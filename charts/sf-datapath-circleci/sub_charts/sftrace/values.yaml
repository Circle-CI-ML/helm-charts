enableDebug: false
archiverEnabled: true
nameOverride: ""
fullnameOverride: ""
image:
  pullPolicy: IfNotPresent
  repository: snappyflowml/sftrace-server
  tag: 'v1-2-60'
imagePullSecrets: []

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""
  # The names of the image pull secrets to be attached to this service account
  imagePullSecrets: []
  # disable mounting sa token inside pods
  automountServiceAccountToken: false

cloud:
  aws:
    enable: true
  gcs:
    enable: false
    # Set all of the below entities when Google-Service-Account is being used. Recommended approach unless K8s is running outside GCP (K8s outside GCP flow which will use JSON service-account-keys is not yet implemented)
    use_google_service_account: true
    service_account: "XXXX-compute@developer.gserviceaccount.com OR XXXX@XXXX.iam.gserviceaccount.com"
    region: us-west1
    zone: us-west1-c
  datacenter:
    enable: false

podSecurityContext:
  {}
  
podAnnotations:
  prometheus.io/scrape: "false"
  
securityContext:
  {}

nodeSelector: {}

tolerations: []

affinity: {}
  
sfapm:
  ingress:
    enabled: false

sfapm_celery:
  env:
    C_FORCE_ROOT: true
    CELERY_OPTS: "-O fair --max-tasks-per-child 20 -P gevent"

  sftrace:
    resources:
      limits:
        cpu: 1000m
        memory: 768Mi
      requests:
        cpu: 10m
        memory: 100Mi
    replicaCount: 1
    command: "celery -A snappyflow worker -l $LOG_LEVEL -Q sftrace -c10 $CELERY_OPTS"

sftrace:
  replicaCount: 2

  terminationGracePeriodSeconds: 30

  maximumQueueEvents: 40000

  # Helm values for sftrace-server-prometheus-exporter container
  sftraceServerPrometheusExporter:
    metricsInterval: 3
    image:
      pullPolicy: Always
    resources:
      requests:
        cpu: 100m
        memory: 200Mi

  podAnnotations:
    prometheus.io/path: /metrics
    prometheus.io/port: "2112"
    prometheus.io/scrape: "true"

  image:
    pullPolicy: Always
  
  resources:
    requests:
      cpu: 10m
      memory: 50Mi
    limits:
      cpu: 1
      memory: 2Gi

  service:
    type: NodePort
    port: 8200
    jaegerport: 14268

  output:
    # Enable either Kafka or Kafkarest as output plugin for trace server
    kafka:
      enabled: true
      # List of Kafka brokers
      hosts: ["kafka-cp-kafka-headless.kafka:9092"]
    kafkarest:
      enabled: false
      host: "sf-datapath-cp-kafka-rest-external.kafka"
      port: 8082
      token: " "

  autoscaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 5
    # This value is calculated as 50 % of max events trace queue can buffer 
    scaleUpPeriod: 5
    scaleDownPeriod: 120

postgresql:
  # enabled true deploys postgresql as statefull set on the same cluster
  enabled: true

  # if enabled false, chart will connect to external database
  # create two data bases with names snappyflow and vizbuilder
  # create a user and grant access to database snappyflow and vizbuilder
  # provide external section if enabled is false

  external:
    dbHost: ""
    dbPort: ""
    dbUser: ""
    dbPassword: ""

  image:
    pullPolicy: IfNotPresent

  # when local postgresql is enabled below values are used to configure database
  rootPassword: postgres
  rootUser: postgres
  multidb: snappyflow;vizbuilder;commands;elasticsearch_manager
  multidbUser: snappyflow
  multidbUserPassord: maplelabs
  
redis:
  image:
    pullPolicy: IfNotPresent
  
global:
  sfappname: sf-portal-app
  sfprojectname: snappyflow-app
  sfappname_key: snappyflow/appname
  sfprojectname_key: snappyflow/projectname
  nginx_geo_info_collection: true
  nginx_ua_parsing: true
  enable_sftrace: false
  key: ""

  sfNodeManager:
    enabled: false
    priorityClassName: sf-critical-pod
  sfScheduler:
    enabled: false
    schedulerName: sf-scheduler